{
    "docs": [
        {
            "location": "/", 
            "text": "Yet Another Hacker News Data Analysis\n\n\n\n\n2020-07-02\n\n\nIntroduction\n\n\nHere I will do some basic analysis of the data provided by \nY-Combinator's social news site, \nHacker News\n (HN).\nThe analysis presented here is a light hearted meandering through different\naspects of the HN site data.\nI've favored an approach that leaves the data interpretation up to the reader\nand kept my own interpretation to a minimum.\n\n\nDoing a search for a snapshot of the HN API data provided some options but nothing\nrecent and nothing with user information also present, at least, not that I could find.\nI decided to scrape what was available from the HN data through their very\nnice \nAPI\n and do some rudimentary analysis.\n\n\nTo get a copy of the database to play around with yourself, a snapshot\nhas been provided as a SQLite file along with the compressed 'raw' JSON files on \nArchive.org\n.\nThe raw data is about 3.6Gb compressed and the SQLite file is 5G compressed and 13G uncompressed.\n\n\nAll source code, unless otherwise stated, as well as text and other data, is provided\nunder a CC0/public domain license.\nThe Hacker News itself is proprietary and so please check with with the\n\nTerms of Use\n.\nI am, no doubt, violating their terms of use so I'm hoping they won't sue me\nor take down the data as I'm providing it in the hacker friendly spirit of\ndata sharing and transparency.\n\n\nThere's nothing novel here, aside, maybe, from an easily downloadable database.\n\n\nThat being said, I imagine a lot of people thinking \"so what?\" and I'd be hard pressed\nto argue with them.\n\n\nSo what? So what! Why not! Let's take a look at Hacker News data!\n\n\nAnalysis\n\n\nOverview\n\n\nFirst let's get an overview of the data:\n\n\nsqlite\n select count(*) from users;\n558905\nsqlite\n select count(*) from item;\n23669105\n\n\n\n\nA breakdown of different types of item data:\n\n\nsqlite\n select count(id), type from item group by type ;\ncount(id)|type\n19882923|comment\n13014|job\n1788|poll\n12164|pollopt\n3759216|story\n\n\n\n\n\n\nPlease note the frequency log-scale.\n\n\n\n\nDifferent item data, grouped by month, through time:\n\n\nsqlite\n select count(id), type, substr(datetime(time, 'unixepoch', 'localtime'), 0, 8) mo  from item group by mo, type order by mo asc;\n...\n255028|comment|2020-04\n52|job|2020-04\n2|poll|2020-04\n7|pollopt|2020-04\n38330|story|2020-04\n296004|comment|2020-05\n63|job|2020-05\n1|poll|2020-05\n9|pollopt|2020-05\n40093|story|2020-05\n259768|comment|2020-06\n71|job|2020-06\n34168|story|2020-06\n\n\n\n\n\n\nUser Item Frequency\n\n\nHow about the frequency of user item creating (polls, comments, stories, etc.), making sure to filter out blank \nby\n fields:\n\n\nsqlite\n select count(id) c, by from item where by != '' group by by order by c desc;\nc|by\n51639|tptacek\n40333|jacquesm\n39999|dang\n34255|rbanffy\n33216|dragonwriter\n31837|pjmlp\n25573|coldtea\n24766|TeMPOraL\n24666|DanBC\n22665|icebraining\n...\n\n\n\n\nEither because my SQL knowledge is substandard or I don't know what I'm doing with the indexes, I found queries took too long in SQL.\nParsing the JSON directly yields faster results for me.\n\n\nAfter some data munging, we get: \n\n\nzcat $datadir/hn-item*.ajson.gz | \\\n  jq -r -c '.by ' | \\\n  grep -P -v '^$' | \\\n  grep -P -v '^null$' | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed 's/^ *//' \n user_item_freq.gp\ncut -f1 -d' ' user_item_freq.gp | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed 's/^ *//'  | \\\n  awk '{ print $2, $1; }' \n user_item_freq_freq.gp\n./zipf-mle user_item_freq.gp \n user_item-exponent-mle.txt\n\n\n\n\nLooking at a log-log plot of the graph:\n\n\n\n\nAh! The tell-tale sign of a power law distribution.\n\n\nHere it is broken out by \nstory\n and \ncomment\n items:\n\n\n\n\nThe y-axis being the number of users which have that frequency of item creation and the x-axis is the number of\nitems created.\nFor example, you can see \ntptacek\n far out on the right, being the only user to have 51639 items created, whereas\nusers that only have 1 comment, say, are much more frequent.\nFor context, I'm at 224 item creation (comments and stories combined) and fall in the middle.\n\n\nThanks to \nJohn D. Cook\n we can copy-pasta\nthe maximum likelihood estimator (MLE) code to give us a value of 1.58 for the exponent for all items, 1.54 for comments and 1.77 for stories.\nAs \na sanity check\n, they all fall in the range of \n(1,3)\n, which is expected\nfor fat-tailed distributions.\n\n\nWhat does it mean? I dunno \n-\\_(:|)_/-\n\n\nUser URLs\n\n\nLet's take a look at the common URLs in the \nabout\n field from the users on HN.\nGetting URLs from the \nabout\n field in the \nusers\n table is difficult to do in vanilla SQLite3 syntax, so excuse\nthe bash incantation:\n\n\nsqlite3 hn.sqlite \n \n( echo 'select about from users where about like \n%https:%\n;' ) | \\\n  sed 's/\n#x2F;/\\//g' | \\\n  grep -o -P 'https?://[a-zA-Z0-9\\.-]*' | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn \n\n\n\n\n\n\nPlease note the log-scale of frequency.\n\n\nOne thing that jumps out is the prevalence of \nkeybase.io\n.\nI can only assume that Y-Combinator has some close relationship to \nkeybase.io\n, maybe going to far as\ninsisting folks going through their program use it.\n\n\nAnother point of note is the high occurrence of sites like \nhackernewsers.com\n.\nAn unkind interpretation would be that some folks are trying to game the HN system\nto give a signal boost to their site.\n\n\nNote that this analysis counts subdomains as separate URLs.\nI don't want to get too far into the weeds with this analysis but your own\ncould include better handling and binning of URLs like \ngithub.io\n or \ntumblr.com\n sites.\n\n\nItem Frequency by Time\n\n\nLet's start with looking at activity of stories and comments during the day.\nSo we're not deluged by extraneous data, let's just take data from Jan 1st 2020 (unixtime \n1577854800\n):\n\n\necho 'select substr(datetime(time, \nunixepoch\n, \nutc\n), 12, 5) t from item where time \n= 1577854800 and time != \n ;' | \\\n  sqlite3 hn.sqlite | \\\n  tr ':' ' ' | \\\n  awk '{ print ($1 \n 10)? ($1+24 + $2/(60)) : ($1+$2/60), $3; }' | \\\n  sort -n \n\n\n\n\n\n\nWhoa! Seems like a pretty clear signal.\n\n\nLet's shift so we see the trend with a bit more clarity:\n\n\necho 'select substr(datetime(time, \nunixepoch\n, \nutc\n), 12, 5) t from item where time \n= 1577854800 and time != \n ;' | \\\n  sqlite3 hn.sqlite | \\\n  tr ':' ' ' | \\\n  awk '{ print ($1 \n 10)? ($1+24 + $2/(60)) : ($1+$2/60), $3; }' | \\\n  sort -n \n\n\n\n\n\n\n16:00 UTC is 09:00 AM PT, so I would guess HN is pretty (US) west coast centric with the majority of activity happening\nat the start of the business day.\n\n\nI wouldn't guess any big surprises, but let's split it out by stories and comments:\n\n\n\n\nNote that the story submissions are much less prevalent than comments and are plotted with different Y axies.\n\n\nIt should be obvious but I only realized this in retrospect from looking at the data that comments have a lag\nbehind stories, for probably obvious reasons.\n\n\n\n\nLet's see if we can pick out any weekend effects.\n\n\necho -e '.separator \n \n\\nselect strftime(\n%w\n, datetime(time, \nunixepoch\n, \nutc\n)), substr(datetime(time, \nunixepoch\n, \nutc\n), 12, 5), type from item where time \n 1577854800 and time != \n;' | \\\n  sqlite3 hn.sqlite \n item-tow.raw\ngrep -P ' (comment|story)$' item-tow.raw | \\\n  cut -f1,2 -d' ' | \\\n  sort  | \\\n  uniq -c | \\\n  sed 's/^ *//' | \\\n  tr ':' ' ' | \\\n  awk '{ print $2 + ($3/24) + ($4/(24*60)), $1; }' | \\\n  sort -n\n\n\n\n\n\n\nAgain, no big surprises.\n\n\nHere, 0 is Sunday and 6 is Saturday (the results of using \n%w\n in \nstrftime\n) and times are in UTC properly rescaled.\n\n\nI would guess most people interact with HN during the weekday and peter off on the weekends, with Sunday being the least active day.\n\n\nStory Word Clouds\n\n\nLet's look at a trigram word cloud for stories in 2019 and 2020 (so far):\n\n\nsqlite3 hn.sqlite \n \n( echo \nselect title  from item where type = 'story' and score \n 30 and score != '' and substr(datetime(time, 'unixepoch', 'utc'), 1,4) = '2019' and time \n= 1546318800 order by id desc;\n ) | \\\n  ./trigrams | \\\n  iconv -f utf-8 -t ascii//translit | \\\n  tr ',-.:!'\n'\n'\n' '_'  \n 2019-filtered.trigrams\n./plot-wordcloud 2019-filtered.trigrams hn-2019-trigram.png\nsqlite3 hn.sqlite \n \n( echo \nselect title  from item where type = 'story' and score \n 30 and score != '' and substr(datetime(time, 'unixepoch', 'utc'), 1,4) = '2020' and time \n= 1577854800 order by id desc;\n ) | \\\n  ./trigrams | \\\n  iconv -f utf-8 -t ascii//translit | \\\n  tr ',-.:!'\n'\n'\n' '_'  \n 2020-filtered.trigrams\n./plot-wordcloud 2020-filtered.trigrams hn-2020-trigram.png\n\n\n\n\n2019:\n\n\n \n\n\n2020 (so far):\n\n\n\n\nPlease excuse the underscores I'm being lazy and constructing \"single\" words by concatenating three words together\nto fool \namueller/word_cloud\n into constructing the word cloud.\n\n\nNote that these are for stories that have a score of more than 30.\nThe mangling of the title might also have an effect, so take care to not read too much into the word cloud.\nAlso note that I don't think there's a way to really get at what stories were on the front page with this data\nso that might skew interpretation.\n\n\nYou can also see some topical subjects from 2019, like the Hong Kong protests, the Boeing 737 and Raspberry Pi 4.\nI guess the take away is there's a lot of 'ask' and 'show' stories that are popular.\nI would interpret this as the community tends to like posts that they can actively engage in.\nIt'll be a cute meta-analysis to see if this post lands on the front page of HN ;).\n\n\nI picked tri-grams pretty arbitrarily.\nSingle words and bi-grams don't really provide enough information to be interesting to me.\nTri-grams look to have just enough interesting information to at least give the illusion\nof insight before being drowned out by the combinatorial explosion of choices.\n\n\nStory Topic Modeling\n\n\nEngaging in some more tea leaf reading, we can do some \ntopic modeling\n.\nTopic modelling is new to me but I discovered it\nfrom the excellent \nblog post on using UNIX for Bi/Tri-Grams\n.\n\n\nRunning a query against 2020 HN stories (so far):\n\n\nsqlite3 hn.sqlite \n \n( echo \nselect title  from item where type = 'story' and score \n 30 and score != ''  and time \n= 1546318800 and time \n= 1609477200; \n ) \n hn-story-2020.txt\nbin/mallet import-file \\\n  --input hn-story-2020.txt \\\n  --output data.mallet \\\n  --keep-sequence \\\n  --remove-stopwords\nbin/mallet train-topics \\\n  --input data.mallet \\\n  --alpha 50.0 \\\n  --beta 0.01 \\\n  --num-topics 100 \\\n  --num-iterations 1000 \\\n  --optimize-interval 10 \\\n  --output-topic-keys data.topic-keys.out \\\n  --topic-word-weights-file data.topic-word-weights.out\ncut -f2- data.topic-keys.out | sort -rn | head -n10\n\n\n\n\n0.06478 library rust javascript python compiler released fast code webassembly part css modern simple small scratch faster framework video written built \n0.06366 work remote people job company employees working workers home hard don\u2019t companies don't culture jobs staff teams hour find made \n0.06013 data users personal tracking science location access phone user private breach facebook customers information passwords records app web collection selling \n0.05564 facebook google ads users privacy twitter account amazon instagram policy accounts back content videos youtube group online whatsapp user concerns \n0.05533 years project year time life side business ago month start work family what's minutes months building past history making today \n0.05478 tech company big students technology startup companies startups surveillance list chinese u.s build fall robot college french online days internet \n0.05056 software engineering developers developer learn guide engineers good engineer code development tools team programmer resources working writing projects learned coding \n0.04906 make money people makes making things internet house don't feel hard made sense big worse living easier bad job claims \n0.04872 web app development build framework apps platform built react design tools modern site open-source applications application server desktop based mobile \n0.04617 coronavirus covid u.s pandemic global fight virus due testing tests response state point lockdown spread test positive emergency reduce set \n...\n\n\n\n\nRust/Javascript/Python/WebAssembly Library/Framework/Compiler?\nRemote work/home workers?\nPersonal data tracking/private phone breach/customer selling breach?\n\n\nSure, sounds like HN, I guess.\n\n\nComment Sentiment Analysis\n\n\nI really don't know how relevant this is to anything but\nwe can run \nvaderSentiment\n on some of the more recent comments.\n\n\necho 'select text from item where type = \ncomment\n order by id desc limit 10000;' | \\\n  sqlite3 hn.sqlite | \\\n  ./sentiment \n\n\n\n\nWhere the program, \nsentiment\n, is copy-pasta'd from one of the examples from \nvaderSentiment\n:\n\n\n#!/usr/bin/python3\nimport sys\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsentences = []\nfor line in sys.stdin:\n  line = line.strip()\n  sentences.append(line)\nanalyzer = SentimentIntensityAnalyzer()\nfor sentence in sentences:\n  vs = analyzer.polarity_scores(sentence)\n  print(\n{:-\n1000} {}\n.format(sentence, str(vs)))\n\n\n\n\nHere's a semi-log plot of the sentiments for the last 10k comments (as of the time of scraping) split out by positive, negative, neutral and \"compound\":\n\n\n\n\nFrom the \nvaderSentiment\n page:\n\n\n\n\nThe compound score is computed ... then normalized to be between -1 (most extreme negative) and +1 (most extreme positive).\nThis is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence.\n\n\n\n\nNot that enlighetening but there it is.\n\n\nIndividual User Sentiment\n\n\nLet's pick on a particular user, \npatio11\n say, and see what we can glean from their comments using the same Topic Modelling script from above.\nSpecifically, we'll run the topic modelling from above on 15 topics:\n\n\nsqlite3 hn.sqlite \n \n( echo 'select type,text from item where by = \npatio11\n and type = \ncomment\n  ;' ) \n patio11.comment\n\n\n\n\n\n0.60089 time day don't work i'm email years back call days business hours you're make week minutes i've phone put customer \n0.59597 people don't you're make good things money lot they're business problem time folks isn't making stuff i'm world problems makes \n0.41363 software business money customers pay month sales price sell people cost buy product costs businesses customer year make selling paying \n0.37823 number data question people answer numbers word case fact e.g actual means results language simple information point words things world \n0.27846 work company job employees working engineers companies salary year clients hire professional employee hiring engineer consulting jobs people years market \n0.21117 code rails web server app user java data application system api users software write apps ruby file database oss e.g \n0.19565 google page site users content a/b testing search seo bingo adwords test conversion ads website user pages internet link don't \n0.16512 you\n#x i\n#x don\n#x they\n#x it\n#x that\n#x we\n#x e.g doesn\n#x people quot quot;i isn\n#x company companies can\n#x didn\n#x things feel f;etc \n0.14622 japanese japan american food live country tokyo americans government town local water chinese poor office space america city car apartment \n0.13561 credit bank business tax money account card pay paypal income taxes insurance year cash payment financial risk payments accountant interest \n0.12715 blog book writing books post write read amazon article email video list posts reading marketing comment topic nyt comments media \n0.11497 legal law security information lawyer system letter account password email address government state states access public policy mail domain check \n0.10383 school game university class students games degree college teachers education english high middle student schools american years math academic blah \n0.08531 bitcoin million company market money stock transaction investors investment shares dollars fund worth funds billion exchange invest returns transactions founders \n0.04306 nofollow rel href f;\n#x https:\n#x http:\n#x http://news.ycombinator.com/item?id http://www.kalzumeus.com f;item?id f;news.ycombinator.com\n#x f;stripe.com\n#x http://images html version pdf gur post read years comment \n\n\n\n\nFrom a blog post by \nluu\n, we know that \npatio11\n is focused on business related comments.\nFrom the word salad above, I would guess \npatio11\n is also interested \nin Japanese culture, some legal issues, maybe some Bitcoin related issues, Bingo, etc.\nFrom \npatio11\n's profile, we see some related Internet, Stripe and Bingo related issues...so, sure?\n\n\nStill seems like reading tea leaves but squinting at it a certain way and you can maybe build a narrative from the above word salad.\n\n\n\n\nFor context here's myself restricted to 5 topics:\n\n\n0.1432  f;\n#x https:\n#x rel nofollow href f;wiki\n#x f;en.wikipedia.org\n#x http:\n#x f;github.com\n#x i\n#x f;album\n#x data it\n#x f;stratfordct.bandcamp.com\n#x music pdf open there\n#x f;www.youtube.com\n#x terms \n0.09759 source open software it\n#x license code hardware data don\n#x i\n#x work making free\n#x design projects make good machine considered f;libre \n0.07996 it\n#x i\n#x you\n#x good people there\n#x time books understand system isn\n#x theory problems tools unix they\n#x that\n#x f;www.amazon.com\n#x find hard \n0.04972 code pre people time poverty energy the\\n ppivnwy?t f;o phi(q we\u2019re population talking order growth pinker function repeated copyright real \n0.03197 distribution power random distributions law laws f;stable_distribution variance finite variables limiting rng sum normal number gaussian independent distributed tails find \n\n\n\n\nFree/libre software? Power law random distributions?\nSure, sort of sounds like me.\n\n\nConclusion\n\n\nThus concludes our meandering tour of the HN dataset.\n\n\nI don't think there's anything new or novel here but it was fun to do.\n\n\nHopefully you've enjoyed it too!\n\n\nSuggestions, comments, PRs, etc. welcome!\n\n\nAppendix: Data Analysis Scripts\n\n\nbigram-freq\n (taken from \nsrc\n):\n\n\n#!/bin/bash\ninfn=\n$1\n\nif [[ \n$infn\n == \n ]] ; then infn=/dev/stdin ; fi\ntmp0=`mktemp`\ntmp1=`mktemp`\ncat $infn | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | sed 's/,//' | sed G | tr ' ' '\\n' \n $tmp0\ntail -n+2 $tmp0 \n $tmp1\npaste -d ',' $tmp0 $tmp1 | grep -v -e \n^,\n | grep -v -e \n,$\n | sort | uniq -c | sort -rn\nrm -f $tmp0 $tmp1\n\n\n\n\nand \ntrigram-freq\n:\n\n\nnfn=\n$1\n\nif [[ \n$infn\n == \n ]] ; then infn=/dev/stdin ; fi\ntmp0=`mktemp`\ntmp1=`mktemp`\ntmp2=`mktemp`\ncat $infn | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | sed 's/,//' | sed G | tr ' ' '\\n' \n $tmp0\ntail -n+2 $tmp0 \n $tmp1\ntail -n+2 $tmp1 \n $tmp2\npaste -d ',' $tmp0 $tmp1 $tmp2 | \\\n  grep -v -e \n^,\n | \\\n  grep -v -e \n,$\n | \\\n  grep -v -e \n,,\n | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn\nrm -f $tmp0 $tmp1 $tmp2\n\n\n\n\nMLE for \nZipf distributions\n (aka \npower law\n tailed distributions)\n (coutesy of \nJohn D. Cook\n):\n\n\n#!/usr/bin/python3\n#\n# \n\nimport re\nimport sys\nfrom scipy import log\nfrom scipy.special import zeta\nfrom scipy.optimize import bisect \n\nimport numpy as np\nimport numpy.random\n\nifn=\n\nif len(sys.argv) \n 1:\n  ifn = sys.argv[1]\n\nif ifn==\n:\n  print(\nprovide input\n)\n  sys.exit(-1)\n\n\n_x = []\nwith open(ifn, \nr\n) as fp:\n  for line in fp:\n    line = line.strip()\n    if len(line) == 0: continue\n    #a = line.split(\n\\t\n)\n    a = re.split('\\t| ', line)\n    f = float(a[0])\n    _x.append(f)\n\n#alpha = 1.5\n#n = 5000\n#x = numpy.random.zipf(alpha, n)\nx = np.array(_x)\n\n#xmin = 1\nxmin = 1\n\ndef log_zeta(x):\n  return log(zeta(x, 1))\n\ndef log_deriv_zeta(x):\n  h = 1e-5\n  return (log_zeta(x+h) - log_zeta(x-h))/(2*h)\n\nt = -sum( log(x/xmin) )/float(len(x))\ndef objective(x):\n  return log_deriv_zeta(x) - t\n\na, b = 1.01, 10\nalpha_hat = bisect(objective, a, b, xtol=1e-6)\nprint(alpha_hat)\n\n\n\n\n\nAppendix: Data Retrieval\n\n\nA combination of C programs and shell scripts were used to scrape the Hacker News (HN) API.\nThe following C programs were used:\n\n\n\n\nhn-parallel-item.c\n - Parallel curl requests (200) for item retrieval\n\n\nhn-parallel-user.c\n - Parallel curl requests (200) for user information retrieval\n\n\nscrape-hn\n - manager script to batch download items and compress them\n\n\nhna\n - helper script to make simple command line HN API requests\n\n\n\n\nThe Curl programs were modified from the Curl examples and their license headers reflect\nthe provenance.\n\n\ncmp.sh\n is the simple script used to compile the two Curl programs.\n\n\nNote that they're all pretty 'hacky' and I only provide them for the sake of transparency.\nGetting them working yourselves probably will require some tinkering.\n\n\nThe parallel requests of 200 were used as this was the maximum parallel requests I could\ndo on my machine without maxing out memory (8Gb).\n\n\nWhen ran, the \nitem\n download took about 6 hrs and the \nuser\n download took about 20 mins.\n\n\nOnce the \nitem\n and \nuser\n information was downloaded, they were converted to tab-delimited\nCSVs using \njq\n along with some \nsed\n magic to properly escape quotes.\n\n\nsql/load-db\n shows the process of loading in the local files.\n\n\nAppendix: Database Schema\n\n\nThis models closely the \nHN API\n.\n\n\ncreate table users (\n  iid INTEGER PRIMARY KEY,\n  id varchar(255),\n  created INTEGER,\n  karma INTEGER\n);\n\ncreate index users_id_idx on users(id);\ncreate index users_created_idx on users(created);\n\ncreate table item (\n  id INTEGER PRIMARY KEY,\n  by varchar(255),\n  dead INTEGER,\n  deleted INTEGER,\n\n  parent INTEGER,\n  poll INTEGER,\n  descendants INTEGER,\n  score INTEGER,\n  text varchar(255),\n  time INTEGER,\n  title varchar(255),\n  type varchar(255),\n  url varchar(255)\n\n);\n\ncreate index item_by_idx on item(by);\ncreate index item_parent_idx on item(parent);\ncreate index item_time_idx on item(time);\ncreate index item_type_idx on item(type);\ncreate index item_time_type_idx on item(time,type);\ncreate index item_time_type_score_idx on item(time,type,score);\n\n\n\n\n\nNote that the \niid\n is a completely synthetic id that hasn't been provided by HN.\n\n\nReferences\n\n\n\n\ngreg.blog: UNIX, Bi-Grams, Tri-Grams, and Topic Modeling\n\n\nMallet\n\n\nMallet: Topic Modeling\n\n\nJohn D. Cook: Estimating the exponent of discrete power law data\n\n\nGithub: Hacker News API\n\n\nWikipedia: Zipf's Law\n\n\nWikipedia: Power Law\n\n\nWikipedia: Levy Stable Distribution\n\n\n\n\nLicense\n\n\nCC0", 
            "title": "Home"
        }, 
        {
            "location": "/#yet-another-hacker-news-data-analysis", 
            "text": "", 
            "title": "Yet Another Hacker News Data Analysis"
        }, 
        {
            "location": "/#2020-07-02", 
            "text": "", 
            "title": "2020-07-02"
        }, 
        {
            "location": "/#introduction", 
            "text": "Here I will do some basic analysis of the data provided by \nY-Combinator's social news site,  Hacker News  (HN).\nThe analysis presented here is a light hearted meandering through different\naspects of the HN site data.\nI've favored an approach that leaves the data interpretation up to the reader\nand kept my own interpretation to a minimum.  Doing a search for a snapshot of the HN API data provided some options but nothing\nrecent and nothing with user information also present, at least, not that I could find.\nI decided to scrape what was available from the HN data through their very\nnice  API  and do some rudimentary analysis.  To get a copy of the database to play around with yourself, a snapshot\nhas been provided as a SQLite file along with the compressed 'raw' JSON files on  Archive.org .\nThe raw data is about 3.6Gb compressed and the SQLite file is 5G compressed and 13G uncompressed.  All source code, unless otherwise stated, as well as text and other data, is provided\nunder a CC0/public domain license.\nThe Hacker News itself is proprietary and so please check with with the Terms of Use .\nI am, no doubt, violating their terms of use so I'm hoping they won't sue me\nor take down the data as I'm providing it in the hacker friendly spirit of\ndata sharing and transparency.  There's nothing novel here, aside, maybe, from an easily downloadable database.  That being said, I imagine a lot of people thinking \"so what?\" and I'd be hard pressed\nto argue with them.  So what? So what! Why not! Let's take a look at Hacker News data!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#analysis", 
            "text": "", 
            "title": "Analysis"
        }, 
        {
            "location": "/#overview", 
            "text": "First let's get an overview of the data:  sqlite  select count(*) from users;\n558905\nsqlite  select count(*) from item;\n23669105  A breakdown of different types of item data:  sqlite  select count(id), type from item group by type ;\ncount(id)|type\n19882923|comment\n13014|job\n1788|poll\n12164|pollopt\n3759216|story   Please note the frequency log-scale.   Different item data, grouped by month, through time:  sqlite  select count(id), type, substr(datetime(time, 'unixepoch', 'localtime'), 0, 8) mo  from item group by mo, type order by mo asc;\n...\n255028|comment|2020-04\n52|job|2020-04\n2|poll|2020-04\n7|pollopt|2020-04\n38330|story|2020-04\n296004|comment|2020-05\n63|job|2020-05\n1|poll|2020-05\n9|pollopt|2020-05\n40093|story|2020-05\n259768|comment|2020-06\n71|job|2020-06\n34168|story|2020-06", 
            "title": "Overview"
        }, 
        {
            "location": "/#user-item-frequency", 
            "text": "How about the frequency of user item creating (polls, comments, stories, etc.), making sure to filter out blank  by  fields:  sqlite  select count(id) c, by from item where by != '' group by by order by c desc;\nc|by\n51639|tptacek\n40333|jacquesm\n39999|dang\n34255|rbanffy\n33216|dragonwriter\n31837|pjmlp\n25573|coldtea\n24766|TeMPOraL\n24666|DanBC\n22665|icebraining\n...  Either because my SQL knowledge is substandard or I don't know what I'm doing with the indexes, I found queries took too long in SQL.\nParsing the JSON directly yields faster results for me.  After some data munging, we get:   zcat $datadir/hn-item*.ajson.gz | \\\n  jq -r -c '.by ' | \\\n  grep -P -v '^$' | \\\n  grep -P -v '^null$' | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed 's/^ *//'   user_item_freq.gp\ncut -f1 -d' ' user_item_freq.gp | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed 's/^ *//'  | \\\n  awk '{ print $2, $1; }'   user_item_freq_freq.gp\n./zipf-mle user_item_freq.gp   user_item-exponent-mle.txt  Looking at a log-log plot of the graph:   Ah! The tell-tale sign of a power law distribution.  Here it is broken out by  story  and  comment  items:   The y-axis being the number of users which have that frequency of item creation and the x-axis is the number of\nitems created.\nFor example, you can see  tptacek  far out on the right, being the only user to have 51639 items created, whereas\nusers that only have 1 comment, say, are much more frequent.\nFor context, I'm at 224 item creation (comments and stories combined) and fall in the middle.  Thanks to  John D. Cook  we can copy-pasta\nthe maximum likelihood estimator (MLE) code to give us a value of 1.58 for the exponent for all items, 1.54 for comments and 1.77 for stories.\nAs  a sanity check , they all fall in the range of  (1,3) , which is expected\nfor fat-tailed distributions.  What does it mean? I dunno  -\\_(:|)_/-", 
            "title": "User Item Frequency"
        }, 
        {
            "location": "/#user-urls", 
            "text": "Let's take a look at the common URLs in the  about  field from the users on HN.\nGetting URLs from the  about  field in the  users  table is difficult to do in vanilla SQLite3 syntax, so excuse\nthe bash incantation:  sqlite3 hn.sqlite    ( echo 'select about from users where about like  %https:% ;' ) | \\\n  sed 's/ #x2F;/\\//g' | \\\n  grep -o -P 'https?://[a-zA-Z0-9\\.-]*' | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn    Please note the log-scale of frequency.  One thing that jumps out is the prevalence of  keybase.io .\nI can only assume that Y-Combinator has some close relationship to  keybase.io , maybe going to far as\ninsisting folks going through their program use it.  Another point of note is the high occurrence of sites like  hackernewsers.com .\nAn unkind interpretation would be that some folks are trying to game the HN system\nto give a signal boost to their site.  Note that this analysis counts subdomains as separate URLs.\nI don't want to get too far into the weeds with this analysis but your own\ncould include better handling and binning of URLs like  github.io  or  tumblr.com  sites.", 
            "title": "User URLs"
        }, 
        {
            "location": "/#item-frequency-by-time", 
            "text": "Let's start with looking at activity of stories and comments during the day.\nSo we're not deluged by extraneous data, let's just take data from Jan 1st 2020 (unixtime  1577854800 ):  echo 'select substr(datetime(time,  unixepoch ,  utc ), 12, 5) t from item where time  = 1577854800 and time !=   ;' | \\\n  sqlite3 hn.sqlite | \\\n  tr ':' ' ' | \\\n  awk '{ print ($1   10)? ($1+24 + $2/(60)) : ($1+$2/60), $3; }' | \\\n  sort -n    Whoa! Seems like a pretty clear signal.  Let's shift so we see the trend with a bit more clarity:  echo 'select substr(datetime(time,  unixepoch ,  utc ), 12, 5) t from item where time  = 1577854800 and time !=   ;' | \\\n  sqlite3 hn.sqlite | \\\n  tr ':' ' ' | \\\n  awk '{ print ($1   10)? ($1+24 + $2/(60)) : ($1+$2/60), $3; }' | \\\n  sort -n    16:00 UTC is 09:00 AM PT, so I would guess HN is pretty (US) west coast centric with the majority of activity happening\nat the start of the business day.  I wouldn't guess any big surprises, but let's split it out by stories and comments:   Note that the story submissions are much less prevalent than comments and are plotted with different Y axies.  It should be obvious but I only realized this in retrospect from looking at the data that comments have a lag\nbehind stories, for probably obvious reasons.   Let's see if we can pick out any weekend effects.  echo -e '.separator    \\nselect strftime( %w , datetime(time,  unixepoch ,  utc )), substr(datetime(time,  unixepoch ,  utc ), 12, 5), type from item where time   1577854800 and time !=  ;' | \\\n  sqlite3 hn.sqlite   item-tow.raw\ngrep -P ' (comment|story)$' item-tow.raw | \\\n  cut -f1,2 -d' ' | \\\n  sort  | \\\n  uniq -c | \\\n  sed 's/^ *//' | \\\n  tr ':' ' ' | \\\n  awk '{ print $2 + ($3/24) + ($4/(24*60)), $1; }' | \\\n  sort -n   Again, no big surprises.  Here, 0 is Sunday and 6 is Saturday (the results of using  %w  in  strftime ) and times are in UTC properly rescaled.  I would guess most people interact with HN during the weekday and peter off on the weekends, with Sunday being the least active day.", 
            "title": "Item Frequency by Time"
        }, 
        {
            "location": "/#story-word-clouds", 
            "text": "Let's look at a trigram word cloud for stories in 2019 and 2020 (so far):  sqlite3 hn.sqlite    ( echo  select title  from item where type = 'story' and score   30 and score != '' and substr(datetime(time, 'unixepoch', 'utc'), 1,4) = '2019' and time  = 1546318800 order by id desc;  ) | \\\n  ./trigrams | \\\n  iconv -f utf-8 -t ascii//translit | \\\n  tr ',-.:!' ' ' ' '_'    2019-filtered.trigrams\n./plot-wordcloud 2019-filtered.trigrams hn-2019-trigram.png\nsqlite3 hn.sqlite    ( echo  select title  from item where type = 'story' and score   30 and score != '' and substr(datetime(time, 'unixepoch', 'utc'), 1,4) = '2020' and time  = 1577854800 order by id desc;  ) | \\\n  ./trigrams | \\\n  iconv -f utf-8 -t ascii//translit | \\\n  tr ',-.:!' ' ' ' '_'    2020-filtered.trigrams\n./plot-wordcloud 2020-filtered.trigrams hn-2020-trigram.png  2019:     2020 (so far):   Please excuse the underscores I'm being lazy and constructing \"single\" words by concatenating three words together\nto fool  amueller/word_cloud  into constructing the word cloud.  Note that these are for stories that have a score of more than 30.\nThe mangling of the title might also have an effect, so take care to not read too much into the word cloud.\nAlso note that I don't think there's a way to really get at what stories were on the front page with this data\nso that might skew interpretation.  You can also see some topical subjects from 2019, like the Hong Kong protests, the Boeing 737 and Raspberry Pi 4.\nI guess the take away is there's a lot of 'ask' and 'show' stories that are popular.\nI would interpret this as the community tends to like posts that they can actively engage in.\nIt'll be a cute meta-analysis to see if this post lands on the front page of HN ;).  I picked tri-grams pretty arbitrarily.\nSingle words and bi-grams don't really provide enough information to be interesting to me.\nTri-grams look to have just enough interesting information to at least give the illusion\nof insight before being drowned out by the combinatorial explosion of choices.", 
            "title": "Story Word Clouds"
        }, 
        {
            "location": "/#story-topic-modeling", 
            "text": "Engaging in some more tea leaf reading, we can do some  topic modeling .\nTopic modelling is new to me but I discovered it\nfrom the excellent  blog post on using UNIX for Bi/Tri-Grams .  Running a query against 2020 HN stories (so far):  sqlite3 hn.sqlite    ( echo  select title  from item where type = 'story' and score   30 and score != ''  and time  = 1546318800 and time  = 1609477200;   )   hn-story-2020.txt\nbin/mallet import-file \\\n  --input hn-story-2020.txt \\\n  --output data.mallet \\\n  --keep-sequence \\\n  --remove-stopwords\nbin/mallet train-topics \\\n  --input data.mallet \\\n  --alpha 50.0 \\\n  --beta 0.01 \\\n  --num-topics 100 \\\n  --num-iterations 1000 \\\n  --optimize-interval 10 \\\n  --output-topic-keys data.topic-keys.out \\\n  --topic-word-weights-file data.topic-word-weights.out\ncut -f2- data.topic-keys.out | sort -rn | head -n10  0.06478 library rust javascript python compiler released fast code webassembly part css modern simple small scratch faster framework video written built \n0.06366 work remote people job company employees working workers home hard don\u2019t companies don't culture jobs staff teams hour find made \n0.06013 data users personal tracking science location access phone user private breach facebook customers information passwords records app web collection selling \n0.05564 facebook google ads users privacy twitter account amazon instagram policy accounts back content videos youtube group online whatsapp user concerns \n0.05533 years project year time life side business ago month start work family what's minutes months building past history making today \n0.05478 tech company big students technology startup companies startups surveillance list chinese u.s build fall robot college french online days internet \n0.05056 software engineering developers developer learn guide engineers good engineer code development tools team programmer resources working writing projects learned coding \n0.04906 make money people makes making things internet house don't feel hard made sense big worse living easier bad job claims \n0.04872 web app development build framework apps platform built react design tools modern site open-source applications application server desktop based mobile \n0.04617 coronavirus covid u.s pandemic global fight virus due testing tests response state point lockdown spread test positive emergency reduce set \n...  Rust/Javascript/Python/WebAssembly Library/Framework/Compiler?\nRemote work/home workers?\nPersonal data tracking/private phone breach/customer selling breach?  Sure, sounds like HN, I guess.", 
            "title": "Story Topic Modeling"
        }, 
        {
            "location": "/#comment-sentiment-analysis", 
            "text": "I really don't know how relevant this is to anything but\nwe can run  vaderSentiment  on some of the more recent comments.  echo 'select text from item where type =  comment  order by id desc limit 10000;' | \\\n  sqlite3 hn.sqlite | \\\n  ./sentiment   Where the program,  sentiment , is copy-pasta'd from one of the examples from  vaderSentiment :  #!/usr/bin/python3\nimport sys\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nsentences = []\nfor line in sys.stdin:\n  line = line.strip()\n  sentences.append(line)\nanalyzer = SentimentIntensityAnalyzer()\nfor sentence in sentences:\n  vs = analyzer.polarity_scores(sentence)\n  print( {:- 1000} {} .format(sentence, str(vs)))  Here's a semi-log plot of the sentiments for the last 10k comments (as of the time of scraping) split out by positive, negative, neutral and \"compound\":   From the  vaderSentiment  page:   The compound score is computed ... then normalized to be between -1 (most extreme negative) and +1 (most extreme positive).\nThis is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence.   Not that enlighetening but there it is.", 
            "title": "Comment Sentiment Analysis"
        }, 
        {
            "location": "/#individual-user-sentiment", 
            "text": "Let's pick on a particular user,  patio11  say, and see what we can glean from their comments using the same Topic Modelling script from above.\nSpecifically, we'll run the topic modelling from above on 15 topics:  sqlite3 hn.sqlite    ( echo 'select type,text from item where by =  patio11  and type =  comment   ;' )   patio11.comment  0.60089 time day don't work i'm email years back call days business hours you're make week minutes i've phone put customer \n0.59597 people don't you're make good things money lot they're business problem time folks isn't making stuff i'm world problems makes \n0.41363 software business money customers pay month sales price sell people cost buy product costs businesses customer year make selling paying \n0.37823 number data question people answer numbers word case fact e.g actual means results language simple information point words things world \n0.27846 work company job employees working engineers companies salary year clients hire professional employee hiring engineer consulting jobs people years market \n0.21117 code rails web server app user java data application system api users software write apps ruby file database oss e.g \n0.19565 google page site users content a/b testing search seo bingo adwords test conversion ads website user pages internet link don't \n0.16512 you #x i #x don #x they #x it #x that #x we #x e.g doesn #x people quot quot;i isn #x company companies can #x didn #x things feel f;etc \n0.14622 japanese japan american food live country tokyo americans government town local water chinese poor office space america city car apartment \n0.13561 credit bank business tax money account card pay paypal income taxes insurance year cash payment financial risk payments accountant interest \n0.12715 blog book writing books post write read amazon article email video list posts reading marketing comment topic nyt comments media \n0.11497 legal law security information lawyer system letter account password email address government state states access public policy mail domain check \n0.10383 school game university class students games degree college teachers education english high middle student schools american years math academic blah \n0.08531 bitcoin million company market money stock transaction investors investment shares dollars fund worth funds billion exchange invest returns transactions founders \n0.04306 nofollow rel href f; #x https: #x http: #x http://news.ycombinator.com/item?id http://www.kalzumeus.com f;item?id f;news.ycombinator.com #x f;stripe.com #x http://images html version pdf gur post read years comment   From a blog post by  luu , we know that  patio11  is focused on business related comments.\nFrom the word salad above, I would guess  patio11  is also interested \nin Japanese culture, some legal issues, maybe some Bitcoin related issues, Bingo, etc.\nFrom  patio11 's profile, we see some related Internet, Stripe and Bingo related issues...so, sure?  Still seems like reading tea leaves but squinting at it a certain way and you can maybe build a narrative from the above word salad.   For context here's myself restricted to 5 topics:  0.1432  f; #x https: #x rel nofollow href f;wiki #x f;en.wikipedia.org #x http: #x f;github.com #x i #x f;album #x data it #x f;stratfordct.bandcamp.com #x music pdf open there #x f;www.youtube.com #x terms \n0.09759 source open software it #x license code hardware data don #x i #x work making free #x design projects make good machine considered f;libre \n0.07996 it #x i #x you #x good people there #x time books understand system isn #x theory problems tools unix they #x that #x f;www.amazon.com #x find hard \n0.04972 code pre people time poverty energy the\\n ppivnwy?t f;o phi(q we\u2019re population talking order growth pinker function repeated copyright real \n0.03197 distribution power random distributions law laws f;stable_distribution variance finite variables limiting rng sum normal number gaussian independent distributed tails find   Free/libre software? Power law random distributions?\nSure, sort of sounds like me.", 
            "title": "Individual User Sentiment"
        }, 
        {
            "location": "/#conclusion", 
            "text": "Thus concludes our meandering tour of the HN dataset.  I don't think there's anything new or novel here but it was fun to do.  Hopefully you've enjoyed it too!  Suggestions, comments, PRs, etc. welcome!", 
            "title": "Conclusion"
        }, 
        {
            "location": "/#appendix-data-analysis-scripts", 
            "text": "bigram-freq  (taken from  src ):  #!/bin/bash\ninfn= $1 \nif [[  $infn  ==   ]] ; then infn=/dev/stdin ; fi\ntmp0=`mktemp`\ntmp1=`mktemp`\ncat $infn | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | sed 's/,//' | sed G | tr ' ' '\\n'   $tmp0\ntail -n+2 $tmp0   $tmp1\npaste -d ',' $tmp0 $tmp1 | grep -v -e  ^,  | grep -v -e  ,$  | sort | uniq -c | sort -rn\nrm -f $tmp0 $tmp1  and  trigram-freq :  nfn= $1 \nif [[  $infn  ==   ]] ; then infn=/dev/stdin ; fi\ntmp0=`mktemp`\ntmp1=`mktemp`\ntmp2=`mktemp`\ncat $infn | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | sed 's/,//' | sed G | tr ' ' '\\n'   $tmp0\ntail -n+2 $tmp0   $tmp1\ntail -n+2 $tmp1   $tmp2\npaste -d ',' $tmp0 $tmp1 $tmp2 | \\\n  grep -v -e  ^,  | \\\n  grep -v -e  ,$  | \\\n  grep -v -e  ,,  | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn\nrm -f $tmp0 $tmp1 $tmp2  MLE for  Zipf distributions  (aka  power law  tailed distributions)\n (coutesy of  John D. Cook ):  #!/usr/bin/python3\n#\n# \n\nimport re\nimport sys\nfrom scipy import log\nfrom scipy.special import zeta\nfrom scipy.optimize import bisect \n\nimport numpy as np\nimport numpy.random\n\nifn= \nif len(sys.argv)   1:\n  ifn = sys.argv[1]\n\nif ifn== :\n  print( provide input )\n  sys.exit(-1)\n\n\n_x = []\nwith open(ifn,  r ) as fp:\n  for line in fp:\n    line = line.strip()\n    if len(line) == 0: continue\n    #a = line.split( \\t )\n    a = re.split('\\t| ', line)\n    f = float(a[0])\n    _x.append(f)\n\n#alpha = 1.5\n#n = 5000\n#x = numpy.random.zipf(alpha, n)\nx = np.array(_x)\n\n#xmin = 1\nxmin = 1\n\ndef log_zeta(x):\n  return log(zeta(x, 1))\n\ndef log_deriv_zeta(x):\n  h = 1e-5\n  return (log_zeta(x+h) - log_zeta(x-h))/(2*h)\n\nt = -sum( log(x/xmin) )/float(len(x))\ndef objective(x):\n  return log_deriv_zeta(x) - t\n\na, b = 1.01, 10\nalpha_hat = bisect(objective, a, b, xtol=1e-6)\nprint(alpha_hat)", 
            "title": "Appendix: Data Analysis Scripts"
        }, 
        {
            "location": "/#appendix-data-retrieval", 
            "text": "A combination of C programs and shell scripts were used to scrape the Hacker News (HN) API.\nThe following C programs were used:   hn-parallel-item.c  - Parallel curl requests (200) for item retrieval  hn-parallel-user.c  - Parallel curl requests (200) for user information retrieval  scrape-hn  - manager script to batch download items and compress them  hna  - helper script to make simple command line HN API requests   The Curl programs were modified from the Curl examples and their license headers reflect\nthe provenance.  cmp.sh  is the simple script used to compile the two Curl programs.  Note that they're all pretty 'hacky' and I only provide them for the sake of transparency.\nGetting them working yourselves probably will require some tinkering.  The parallel requests of 200 were used as this was the maximum parallel requests I could\ndo on my machine without maxing out memory (8Gb).  When ran, the  item  download took about 6 hrs and the  user  download took about 20 mins.  Once the  item  and  user  information was downloaded, they were converted to tab-delimited\nCSVs using  jq  along with some  sed  magic to properly escape quotes.  sql/load-db  shows the process of loading in the local files.", 
            "title": "Appendix: Data Retrieval"
        }, 
        {
            "location": "/#appendix-database-schema", 
            "text": "This models closely the  HN API .  create table users (\n  iid INTEGER PRIMARY KEY,\n  id varchar(255),\n  created INTEGER,\n  karma INTEGER\n);\n\ncreate index users_id_idx on users(id);\ncreate index users_created_idx on users(created);\n\ncreate table item (\n  id INTEGER PRIMARY KEY,\n  by varchar(255),\n  dead INTEGER,\n  deleted INTEGER,\n\n  parent INTEGER,\n  poll INTEGER,\n  descendants INTEGER,\n  score INTEGER,\n  text varchar(255),\n  time INTEGER,\n  title varchar(255),\n  type varchar(255),\n  url varchar(255)\n\n);\n\ncreate index item_by_idx on item(by);\ncreate index item_parent_idx on item(parent);\ncreate index item_time_idx on item(time);\ncreate index item_type_idx on item(type);\ncreate index item_time_type_idx on item(time,type);\ncreate index item_time_type_score_idx on item(time,type,score);  Note that the  iid  is a completely synthetic id that hasn't been provided by HN.", 
            "title": "Appendix: Database Schema"
        }, 
        {
            "location": "/#references", 
            "text": "greg.blog: UNIX, Bi-Grams, Tri-Grams, and Topic Modeling  Mallet  Mallet: Topic Modeling  John D. Cook: Estimating the exponent of discrete power law data  Github: Hacker News API  Wikipedia: Zipf's Law  Wikipedia: Power Law  Wikipedia: Levy Stable Distribution", 
            "title": "References"
        }, 
        {
            "location": "/#license", 
            "text": "CC0", 
            "title": "License"
        }
    ]
}